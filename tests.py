# test_cls_token_extraction.py
# å®Œæ•´æµ‹è¯• VisionLanguageModel ä¸­ [CLS] token çš„æå–é€»è¾‘

import torch
import os
from models.vision_language_model import VisionLanguageModel
from models.config import VLMConfig
from transformers import AutoTokenizer

print("ğŸ§ª å¼€å§‹æµ‹è¯•ï¼š[CLS] token æå–é€»è¾‘ï¼ˆå·²ä¿®å¤ pad_token é—®é¢˜ï¼‰")

# ==================================================
# 1. åŠ è½½é…ç½®ä¸ tokenizer
# ==================================================

cfg = VLMConfig()
tokenizer_name = cfg.lm_tokenizer  # 'HuggingFaceTB/cosmo2-tokenizer'

print(f"Loading tokenizer: {tokenizer_name}")
try:
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
except Exception as e:
    print(f"âŒ æ— æ³•åŠ è½½ tokenizerï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç¼“å­˜")
    raise e

# --------------------------------------------------
# âœ… å…³é”®ä¿®å¤ï¼šè®¾ç½® pad_token
# --------------------------------------------------
if tokenizer.pad_token is None:
    if tokenizer.eos_token is not None:
        tokenizer.pad_token = tokenizer.eos_token
        print(f"ğŸ”§ å·²è®¾ç½® pad_token = eos_token ('{tokenizer.eos_token}')")
    else:
        # å¦‚æœè¿ eos_token éƒ½æ²¡æœ‰ï¼Œå°±æ·»åŠ ä¸€ä¸ª
        tokenizer.add_special_tokens({'eos_token': '</s>'})
        tokenizer.pad_token = tokenizer.eos_token
        print(f"ğŸ”§ å·²è¡¥å…… eos_token å’Œ pad_token (token='</s>')")

print(f"âœ… pad_token = '{tokenizer.pad_token}', id={tokenizer.pad_token_id}")

# --------------------------------------------------
# âœ… ç¡®ä¿ [CLS] token å­˜åœ¨
# --------------------------------------------------
if "[CLS]" not in tokenizer.get_vocab():
    num_added = tokenizer.add_tokens(["[CLS]"])
    print(f"ğŸŸ¢ æˆåŠŸæ·»åŠ  [CLS] token (å…±æ–°å¢ {num_added} ä¸ª token)")
else:
    print("ğŸŸ¢ [CLS] å·²å­˜åœ¨äºè¯æ±‡è¡¨ä¸­")

# æ›´æ–° vocab size åˆ° configï¼ˆæ¨¡æ‹Ÿ resize å‰çŠ¶æ€ï¼‰
original_vocab_size = len(tokenizer)

# ==================================================
# 2. åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆä¸åŠ è½½ backbone æƒé‡ï¼‰
# ==================================================

print("\nğŸ§  åˆ›å»º VisionLanguageModel å®ä¾‹...")
model = VisionLanguageModel(cfg, load_backbone=False)  # æ— éœ€åŠ è½½é¢„è®­ç»ƒæƒé‡
model.decoder.resize_token_embeddings(len(tokenizer))
model.eval()

# ğŸ” å¦‚æœä½ å·²ç»å®ç°äº† resize_token_embeddingsï¼Œè¯·å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä¸¤è¡Œï¼š
# print(f"ğŸ”„ è°ƒç”¨ decoder.resize_token_embeddings({len(tokenizer)})")
# model.decoder.resize_token_embeddings(len(tokenizer))

print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
print(f"   åˆ†ç±»å¤´ç»“æ„: {model.classifier}")

print("\n" + "="*60)
print("1ï¸âƒ£ æµ‹è¯• forward å‡½æ•°ä¸­çš„ [CLS] æå–")
print("="*60)

# ==================================================
# æ„é€ æµ‹è¯•è¾“å…¥
# ==================================================

texts = [
    "[CLS] A cat is sitting on the grass.",
    "[CLS] An urban cityscape with tall buildings at night."
]

# ä½¿ç”¨ tokenizer ç¼–ç ï¼Œå¹¶å¯ç”¨ padding/truncation
inputs = tokenizer(
    texts,
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=512  # é¿å… warning
)

input_ids = inputs["input_ids"]         # [2, T]
attention_mask = inputs["attention_mask"]  # [2, T]

# ==================================================
# æ‰“å°å¹¶éªŒè¯è¾“å…¥æ ¼å¼
# ==================================================

print("ğŸ“ è¾“å…¥æ–‡æœ¬:")
for i, text in enumerate(texts):
    print(f"  [{i}] {text}")

print(f"\nğŸ”¢ input_ids.shape: {tuple(input_ids.shape)}")
print(f"ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ª token ID: {input_ids[0, 0].item()}")
print(f"[CLS] çš„ token ID: {tokenizer.convert_tokens_to_ids('[CLS]')}")

assert input_ids[0, 0].item() == tokenizer.convert_tokens_to_ids("[CLS]") and \
       input_ids[1, 0].item() == tokenizer.convert_tokens_to_ids("[CLS]"), \
    "âŒ é”™è¯¯ï¼šè¾“å…¥æœªä»¥ [CLS] å¼€å¤´"

print("âœ… æ‰€æœ‰è¾“å…¥å‡ä»¥ [CLS] å¼€å¤´ âœ”ï¸")

# ==================================================
# å‰å‘ä¼ æ’­æµ‹è¯•
# ==================================================

images = torch.randn(2, 3, 224, 224)  # B=2, C=3, H=224, W=224

with torch.no_grad():
    lm_logits, total_loss, class_logits = model(
        input_ids=input_ids,
        image=images,
        attention_mask=attention_mask,
        targets=input_ids.clone(),      # mock target for gen loss
        targets_cls=torch.tensor([0, 2])  # fake labels for classification
    )

print(f"\nğŸ” è¾“å‡ºå½¢çŠ¶:")
print(f"  lm_logits.shape     : {tuple(lm_logits.shape)}")
print(f"  total_loss          : {total_loss.item():.4f}")
print(f"  class_logits.shape  : {tuple(class_logits.shape)} â†’ åº”ä¸º (2, 3)")

assert class_logits.shape == (2, 3), "åˆ†ç±» logits å½¢çŠ¶é”™è¯¯"
print("âœ… class_logits å½¢çŠ¶æ­£ç¡® âœ…")

# ==================================================
# æ‰‹åŠ¨å¤ç° cls_position æå–è¿‡ç¨‹ï¼ˆè°ƒè¯•ç”¨ï¼‰
# ==================================================

image_embd = model.vision_encoder(images)
image_embd = model.MP(image_embd)
img_seq_len = image_embd.size(1)

print(f"\nğŸ“Š å›¾åƒ token åºåˆ—é•¿åº¦: {img_seq_len}")
print(f"cls_position = img_seq_len = {img_seq_len}")

token_embd = model.decoder.token_embedding(input_ids)
combined_embd = torch.cat((image_embd, token_embd), dim=1)

# è·å–éšè—çŠ¶æ€
hidden_states = model.decoder(combined_embd, attention_mask)

# æ‰‹åŠ¨æå– [CLS] è¡¨ç¤º
cls_hidden_state = hidden_states[:, img_seq_len:img_seq_len+1, :]  # [2,1,D]
manual_class_logits = model.classifier(cls_hidden_state).squeeze(1)

diff = (manual_class_logits - class_logits).abs().max()
print(f"æ‰‹åŠ¨è®¡ç®— vs æ¨¡å‹å†…éƒ¨è¾“å‡ºæœ€å¤§å·®å¼‚: {diff:.6f}")
assert diff < 1e-5, "æ¨ç†ç»“æœä¸ä¸€è‡´"

print("ğŸŸ¢ æ‰‹åŠ¨éªŒè¯é€šè¿‡ï¼[CLS] æå–é€»è¾‘å®Œå…¨æ­£ç¡®ã€‚")

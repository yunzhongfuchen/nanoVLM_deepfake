from transformers import AutoTokenizer
import torchvision.transforms as transforms

TOKENIZERS_CACHE = {}

def get_tokenizer(name):
    if name not in TOKENIZERS_CACHE:
        tokenizer = AutoTokenizer.from_pretrained(name, use_fast=True)
        tokenizer.pad_token = tokenizer.eos_token
        # æ˜¾å¼æ£€æŸ¥æ¯ä¸ª token æ˜¯å¦å·²å­˜åœ¨
        new_tokens = ["<CLS>", "<SEG>"]
        tokens_to_add = [t for t in new_tokens if t not in tokenizer.get_vocab()]

        if tokens_to_add:
            num_added = tokenizer.add_special_tokens({"additional_special_tokens": tokens_to_add})
            print(f"âœ… æ·»åŠ äº† {num_added} ä¸ªæ–°ç‰¹æ®Š token: {tokens_to_add}")
        else:
            print("ğŸŸ¢ æ‰€éœ€ token å·²å­˜åœ¨ï¼Œæ— éœ€æ·»åŠ ")

        print(f"æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")
        TOKENIZERS_CACHE[name] = tokenizer
    return TOKENIZERS_CACHE[name]

def get_image_processor(img_size):
    return transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor()
    ])

def get_mask_processor(img_size):
    """ç”¨äºäºŒå€¼æ©ç ï¼ˆtampering maskï¼‰çš„é¢„å¤„ç†å™¨"""
    return transforms.Compose([
        transforms.Resize((img_size, img_size), interpolation=transforms.InterpolationMode.NEAREST),
        transforms.ToTensor(),  # [0, 255] -> [0, 1], shape (1, H, W)
        transforms.Lambda(lambda x: (x > 0.5).float())  # å¼ºåˆ¶äºŒå€¼åŒ–ä¸º 0.0 / 1.0
    ])